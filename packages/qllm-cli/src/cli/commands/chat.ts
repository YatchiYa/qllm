// src/commands/chat.ts

import { Command } from "commander";
import prompts from "prompts";
import { cliOptions } from "../options";
import { logger } from "@qllm-lib/common/utils/logger";
import { ErrorManager } from "@qllm-lib/common/utils/error_manager";
import { resolveModelAlias } from "@qllm-lib/config/model_aliases";
import { ProviderFactory } from "@qllm-lib/core/providers/provider_factory";
import { configManager } from "@qllm-lib/config/configuration_manager";
import { DEFAULT_APP_CONFIG } from "@qllm-lib/config/default_config";
import { ProviderName } from "@qllm/types/src";
import { displayOptions } from "@qllm-lib/common/utils/option_display";
import { LLMProviderOptions, Message } from '@qllm/types/src';
import { handleStreamWithSpinner } from "@qllm-lib/common/utils/stream_helper";
import { Spinner } from "../../helpers/spinner";
import { ErrorHandler } from '@qllm-lib/common/utils/error_handler';
import { QllmError } from '@qllm-lib/common/errors/custom_errors';

/**
 * Creates and returns the 'chat' command for the CLI application.
 * This command initiates an interactive chat session with a Language Learning Model (LLM).
 * 
 * @returns {Command} The configured 'chat' command
 */
export function createChatCommand(): Command {
  const chatCommand = new Command("chat")
    .description("Start an interactive chat session with the LLM")
    .addOption(cliOptions.maxTokensOption)
    .addOption(cliOptions.temperatureOption)
    .addOption(cliOptions.topPOption)
    .addOption(cliOptions.topKOption)
    .addOption(cliOptions.systemOption)
    .action(async (options, command) => {
      try {
        // Retrieve configuration
        const config = configManager.getConfig();
        const parentOptions = command.parent.opts();

        // Set AWS profile and region if provided
        if (parentOptions.profile) {
          process.env.AWS_PROFILE = parentOptions.profile;
        }
        if (parentOptions.region) {
          process.env.AWS_REGION = parentOptions.region;
        }

        // Resolve model and provider
        const modelAlias = (parentOptions.model as string) || config.defaultModelAlias;
        const providerName = ((parentOptions.provider as string) ||
          config.defaultProvider ||
          DEFAULT_APP_CONFIG.defaultProvider) as ProviderName;
        
        // Log debug information
        logger.debug(`modelAlias: ${modelAlias}`);
        logger.debug(`providerName: ${providerName}`);
        logger.debug(`defaultProviderName: ${config.defaultProvider}`);
        
        // Resolve model alias to model id
        const modelId = parentOptions.modelId || modelAlias
          ? resolveModelAlias(providerName, modelAlias)
          : config.defaultModelId;

        if (!modelId) {
          ErrorManager.throwError("ModelError", `Model id ${modelId} not found`);
        }

        const maxTokens = options.maxTokens || config.defaultMaxTokens;

        logger.debug(`modelId: ${modelId}`);
        logger.debug(`maxTokens: ${maxTokens}`);

        // Get the provider
        const provider = await ProviderFactory.getProvider(providerName);

        // Initialize messages array for the chat session
        const messages: Message[] = [];

        logger.info('Starting chat session. Type "exit" to end the session.');

        // Prepare LLM options
        const llmOptions: LLMProviderOptions = {
          maxTokens: maxTokens,
          temperature: options.temperature,
          topP: options.topP,
          topK: options.topK,
          model: modelId,
        };

        logger.debug(`providerName: ${providerName}`);
        displayOptions(llmOptions, "chat");

        // Main chat loop
        while (true) {
          // Prompt user for input
          const response = await prompts({
            type: "text",
            name: "input",
            message: "You:",
          });

          // Check if user wants to exit
          if (response.input.toLowerCase() === "exit") {
            logger.info("Chat session ended.");
            break;
          }

          // Add user message to the conversation
          messages.push({ role: "user", content: response.input });

          // Generate and display AI response
          const fullResponse = await handleStreamWithSpinner(
            provider,
            messages,
            llmOptions, 
            new Spinner("Generating response...")
          );

          // Display the response on the console
          const formattedResponse = `ðŸ¤– : ${fullResponse}`;
          console.log(formattedResponse);

          // Add AI response to the conversation
          messages.push({ role: "assistant", content: fullResponse });
        }
      } catch (error) {
        // Handle errors
        if (error instanceof QllmError) {
          ErrorHandler.handle(error);
        } else {
          ErrorHandler.handle(new QllmError(`Unexpected error in chat command: ${error}`));
        }
        process.exit(1);
      }
    });

  return chatCommand;
}

export default createChatCommand;